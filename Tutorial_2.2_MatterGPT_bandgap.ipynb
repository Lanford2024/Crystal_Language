{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582b3e45",
   "metadata": {},
   "source": [
    "# Build training sets (skip this part if you have already completed Tutorial_2.1)\n",
    "**!!! We strongly recommend using our Docker image for the setup of Jupyter backend.** \n",
    "**!!! If you choose not to use the Docker image, you'll need to install Slurm on your local machine, which can be tricky.** \n",
    "**!!! If you don't want to install the Slurm workload manager, you'll need to modify the code in utils.py, replacing 'qsub 0_run.pbs' with 'python 0_run.py' inside the splitRun function. Additionally, please ensure that the number of threads does not exceed the number of CPU threads on your computer. Exceeding this limit may lead to resource contention issues.**\n",
    "**!!! 优先使用我们提供的Docker镜像运行Jupyter后端.** \n",
    "**!!! 如果您不想使用Docker镜像，则需要在本机上安装Slurm任务管理系统，这可能会比较复杂。请注意，在运行Jupyter后端之前，需要完成Slurm的安装,否则计算会报错.** \n",
    "**!!! 如果您不想安装Slurm任务管理系统，那么需要修改utils.py的代码，在splitRun函数内部替换 qsub 0_run.pbs为 python 0_run.py，并且确认线程数不会超过电脑的cpu线程数量，否则会出现计算资源挤占的问题.** \n",
    "\n",
    "https://github.com/xiaohang007/SLICES?tab=readme-ov-file#jupyter-backend-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b72c84",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from slices.utils import temporaryWorkingDirectory,splitRun,show_progress,collect_json,collect_csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "def stratified_split(data, target_column, test_size=0.1, random_state=42):\n",
    "    non_numeric = data[pd.to_numeric(data[target_column], errors='coerce').isna()]\n",
    "    if not non_numeric.empty:\n",
    "        print(\"以下行无法转换为数值类型:\")\n",
    "        print(non_numeric)\n",
    "        print(\"\\n无法转换的唯一值:\")\n",
    "        print(non_numeric[target_column].unique())\n",
    "    data[target_column] = pd.to_numeric(data[target_column], errors='coerce')\n",
    "    data_cleaned = data.dropna(subset=[target_column])\n",
    "    print(f\"\\n原始数据行数: {len(data)}\")\n",
    "    print(f\"清理后数据行数: {len(data_cleaned)}\")\n",
    "    data_cleaned['bin'] = pd.cut(data_cleaned[target_column], \n",
    "                         bins=[-np.inf, 0, 0.5, 1, 2, np.inf], \n",
    "                         labels=['zero', 'low', 'medium', 'high', 'very_high'])\n",
    "    train_data = pd.DataFrame(columns=data_cleaned.columns)\n",
    "    test_data = pd.DataFrame(columns=data_cleaned.columns)\n",
    "    for bin_label in data_cleaned['bin'].unique():\n",
    "        bin_data = data_cleaned[data_cleaned['bin'] == bin_label]\n",
    "        if len(bin_data) > 1:\n",
    "            bin_train, bin_test = train_test_split(bin_data, test_size=test_size, random_state=random_state)\n",
    "        else:\n",
    "            bin_train, bin_test = bin_data, pd.DataFrame()\n",
    "        train_data = pd.concat([train_data, bin_train])\n",
    "        test_data = pd.concat([test_data, bin_test])\n",
    "    train_data = train_data.drop('bin', axis=1)\n",
    "    test_data = test_data.drop('bin', axis=1)\n",
    "    train_data = train_data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with temporaryWorkingDirectory(\"./data/mp20_nonmetal/\"):\n",
    "        output=[]\n",
    "        data_path_predix=\"../mp20/\"\n",
    "        data=pd.read_csv(data_path_predix+\"test.csv\")\n",
    "        cifs=list(data[\"cif\"])\n",
    "        ids=list(data[\"material_id\"])\n",
    "        eform=list(data[\"formation_energy_per_atom\"])\n",
    "        bandgap=list(data[\"band_gap\"])\n",
    "        for i in range(len(ids)):\n",
    "            output.append({\"material_id\":ids[i],\"formation_energy_per_atom\":eform[i],\"cif\":cifs[i],\"band_gap\":bandgap[i]})\n",
    "        data=pd.read_csv(data_path_predix+\"val.csv\")\n",
    "        cifs=list(data[\"cif\"])\n",
    "        ids=list(data[\"material_id\"])\n",
    "        eform=list(data[\"formation_energy_per_atom\"])\n",
    "        bandgap=list(data[\"band_gap\"])\n",
    "        for i in range(len(ids)):\n",
    "            output.append({\"material_id\":ids[i],\"formation_energy_per_atom\":eform[i],\"cif\":cifs[i],\"band_gap\":bandgap[i]})\n",
    "        data=pd.read_csv(data_path_predix+\"train.csv\")\n",
    "        cifs=list(data[\"cif\"])\n",
    "        ids=list(data[\"material_id\"])\n",
    "        eform=list(data[\"formation_energy_per_atom\"])\n",
    "        bandgap=list(data[\"band_gap\"])\n",
    "        for i in range(len(ids)):\n",
    "            output.append({\"material_id\":ids[i],\"formation_energy_per_atom\":eform[i],\"cif\":cifs[i],\"band_gap\":bandgap[i]})\n",
    "        with open('cifs.json', 'w') as f:\n",
    "            json.dump(output, f)\n",
    "        splitRun(filename='./cifs.json',threads=16,skip_header=False)\n",
    "        show_progress()\n",
    "        collect_json(output=\"cifs_filtered.json\", \\\n",
    "            glob_target=\"./**/output.json\",cleanup=False)\n",
    "        collect_csv(output=\"mp20_eform_bandgap_nonmetal.csv\", \\\n",
    "            glob_target=\"./**/result.csv\",cleanup=True,header=\"SLICES,eform,bandgap\\n\")\n",
    "        os.system(\"rm cifs.json\")\n",
    "        data = pd.read_csv('mp20_eform_bandgap_nonmetal.csv')\n",
    "        target_column = data.columns[-1]  # 假设最后一列是目标值\n",
    "        train_data, test_data = stratified_split(data, target_column)\n",
    "        print(train_data[target_column].value_counts(bins=5, normalize=True))\n",
    "        print(test_data[target_column].value_counts(bins=5, normalize=True))\n",
    "        train_data.to_csv('train_data_reduce_zero.csv', index=False)\n",
    "        test_data.to_csv('test_data_reduce_zero.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8997fc",
   "metadata": {},
   "source": [
    "# Train MatterGPT for Single-Property Material Inverse Design (using band gap as an example)\n",
    "<span style=\"color:red\">**CUDA in xiaohang07/slices:chgnet2 Docker works on WSL@Windows 11 but may fail on Ubuntu; for Ubuntu, use host machine to train and generate SLICES with MatterGPT.**</span>\n",
    "\n",
    "**Set --epochs 5 to speed up the test run.**\n",
    "\n",
    "**To accelerate the training process, consider adjusting the batch_size appropriately.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5295e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from slices.utils import temporaryWorkingDirectory\n",
    "import os\n",
    "with temporaryWorkingDirectory(\"./MatterGPT/bandgap/1_train_generate\"):\n",
    "    os.system('''\n",
    "    /bin/bash -c \"source /opt/conda/etc/profile.d/conda.sh && conda activate chgnet && \\\n",
    "    python train.py --run_name bandgap_Aug1 --batch_size 12 --num_props 1 --max_epochs 5 --n_embd 768  --n_layer 12 --n_head 12 --learning_rate 1e-4 \\\n",
    "    --train_dataset '../../../data/mp20_nonmetal/train_data_reduce_zero.csv' --test_dataset '../../../data/mp20_nonmetal/test_data_reduce_zero.csv' \"\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff1c75",
   "metadata": {},
   "source": [
    "# Generate SLICES strings with specified $E_{gap}$ = [1,2,3,4]  eV/atom\n",
    "<span style=\"color:red\">**CUDA in xiaohang07/slices:chgnet2 Docker works on WSL@Windows 11 but may fail on Ubuntu; for Ubuntu, use host machine to train and generate SLICES with MatterGPT.**</span>\n",
    "\n",
    "**Set --gen_size 5 to speed up the test run.**\n",
    "\n",
    "**To accelerate the generation process, consider adjusting the batch_size appropriately.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c5226a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from slices.utils import temporaryWorkingDirectory\n",
    "import os\n",
    "with temporaryWorkingDirectory(\"./MatterGPT/bandgap/1_train_generate\"):\n",
    "    os.system('''\n",
    "    /bin/bash -c \"source /opt/conda/etc/profile.d/conda.sh && conda activate chgnet && \\\n",
    "    python generate.py --model_weight bandgap_Aug1.pt --prop_targets \"[1.0, 2.0, 3.0, 4.0]\" --gen_size 5 --batch_size 5 --csv_name inverse_designed_SLICES --n_head 12\"\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730a8ee",
   "metadata": {},
   "source": [
    "# Reconstruct crystals from SLICES and assess novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9fd23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from slices.utils import temporaryWorkingDirectory,splitRun_csv,show_progress,collect_csv\n",
    "import os\n",
    "import glob\n",
    "from slices.utils import splitRun_csv, show_progress, collect_csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle,json\n",
    "from pymatgen.core.structure import Structure\n",
    "def load_and_save_structure_database():\n",
    "    with open('../../../data/mp20_nonmetal/cifs_filtered.json', 'r') as f:\n",
    "        cifs = json.load(f)\n",
    "    \n",
    "    structure_database = []\n",
    "    for i in range(len(cifs)):\n",
    "        cif_string = cifs[i][\"cif\"]\n",
    "        try:\n",
    "            stru = Structure.from_str(cif_string, \"cif\")\n",
    "            structure_database.append([stru, cifs[i][\"band_gap\"]])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Serialize the data\n",
    "    with open('structure_database.pkl', 'wb') as f:\n",
    "        pickle.dump(structure_database, f)\n",
    "        \n",
    "def process_data():\n",
    "    load_and_save_structure_database()\n",
    "    splitRun_csv(filename='../1_train_generate/inverse_designed_SLICES.csv', threads=10, skip_header=True)\n",
    "    show_progress()\n",
    "    collect_csv(output=\"results.csv\",\n",
    "                glob_target=\"./job_*/result.csv\", cleanup=True,\n",
    "                header=\"bandgap_target,SLICES,poscar,novelty\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "    with temporaryWorkingDirectory(\"./MatterGPT/bandgap/2_inverse_novelty\"):\n",
    "        process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408073a",
   "metadata": {},
   "source": [
    "# Evaluate the formation energy distribution of the reconstructed crystals at PBE level (need workstation or even HPC to run VASP fastly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a6878-c05d-4d70-8e4b-30d920f6446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "cd ./MatterGPT/bandgap/3_DFT\n",
    "python 1_run.py\n",
    "# done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70447d29-a36c-4d20-802c-d8857ee49bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
